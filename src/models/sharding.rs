//! Phased model loading for wasm32 browser deployment.
//!
//! # The Problem
//!
//! The Q8-quantized Voxtral model is ~4.2 GB, which exceeds wasm32's 4 GiB
//! linear memory limit when accounting for runtime overhead. Loading the full
//! model at once is not feasible.
//!
//! # Solution: Phased Inference
//!
//! The encoder (~0.7 GB) and decoder (~3.5 GB) are never needed simultaneously.
//! By loading them in sequence, peak memory = max(encoder, decoder) ≈ 3.6 GB,
//! which fits within wasm32 with ~400 MB headroom.
//!
//! ## Phase 1: Encode
//! 1. Load encoder shard (~0.7 GB) + adapter shard (~30 MB)
//! 2. Run audio through mel → encoder → reshape → adapter
//! 3. Store audio_embeds tensor (~1.5 MB for 15s audio)
//! 4. Drop encoder + adapter (frees ~0.7 GB)
//!
//! ## Phase 2: Decode
//! 1. Load decoder shard (~3.5 GB)
//! 2. Run autoregressive generation using stored audio_embeds
//! 3. Decode tokens to text
//! 4. Drop decoder
//!
//! Peak memory: ~3.6 GB (decoder + KV cache + overhead) — fits wasm32.
//!
//! # Shard Files
//!
//! Generated by `quantize_model shard` from a Q8-quantized full model:
//! - `encoder.mpk.gz`  — AudioEncoder module (~0.7 GB)
//! - `adapter.mpk.gz`  — AudioLanguageAdapter module (~30 MB)
//! - `decoder.mpk.gz`  — LanguageModel module (~3.5 GB)

use anyhow::Result;
use burn::module::Module;
use burn::prelude::Backend;
use burn::record::{FullPrecisionSettings, NamedMpkGzFileRecorder};
use std::path::Path;

use super::adapter::{reshape_encoder_output, AudioLanguageAdapter};
use super::decoder::LanguageModel;
use super::encoder::AudioEncoder;
use super::voxtral::{VoxtralModel, VoxtralModelConfig};

/// Phased shard types.
#[derive(Debug, Clone, Copy, PartialEq, Eq)]
pub enum PhasedShard {
    /// Audio encoder (32 transformer layers + conv downsampler)
    Encoder,
    /// Audio-to-LLM adapter projection (2 linear layers)
    Adapter,
    /// Language model decoder (embeddings + 26 layers + norm)
    Decoder,
}

impl PhasedShard {
    pub fn filename(&self) -> &'static str {
        match self {
            PhasedShard::Encoder => "encoder",
            PhasedShard::Adapter => "adapter",
            PhasedShard::Decoder => "decoder",
        }
    }

    pub fn all() -> &'static [PhasedShard] {
        &[
            PhasedShard::Encoder,
            PhasedShard::Adapter,
            PhasedShard::Decoder,
        ]
    }
}

/// Save a model as phased shards for browser deployment.
///
/// Decomposes the model into encoder, adapter, and decoder, saving each
/// as a separate .mpk.gz file that can be loaded independently.
///
/// When `streamed` is true, the decoder is saved as 28 individual files
/// (embeddings + 26 layers + norm) instead of one monolithic shard.
/// This keeps per-step peak memory under the wasm32 4 GiB limit.
pub fn save_phased_shards<B: Backend>(
    model: VoxtralModel<B>,
    output_dir: &Path,
    streamed: bool,
) -> Result<()> {
    std::fs::create_dir_all(output_dir)?;

    let (encoder, decoder, adapter, _reshape_factor) = model.into_parts();
    let recorder = NamedMpkGzFileRecorder::<FullPrecisionSettings>::default();

    println!("Saving encoder shard...");
    encoder
        .save_file(output_dir.join("encoder"), &recorder)
        .map_err(|e| anyhow::anyhow!("Failed to save encoder: {}", e))?;
    report_size(&output_dir.join("encoder.mpk.gz"));

    println!("Saving adapter shard...");
    adapter
        .save_file(output_dir.join("adapter"), &recorder)
        .map_err(|e| anyhow::anyhow!("Failed to save adapter: {}", e))?;
    report_size(&output_dir.join("adapter.mpk.gz"));

    if streamed {
        println!("Saving decoder as streamed shards (per-layer)...");
        save_streamed_decoder_shards(decoder, output_dir)?;
    } else {
        println!("Saving decoder shard...");
        decoder
            .save_file(output_dir.join("decoder"), &recorder)
            .map_err(|e| anyhow::anyhow!("Failed to save decoder: {}", e))?;
        report_size(&output_dir.join("decoder.mpk.gz"));
    }

    // Write manifest
    let manifest = ShardManifest::from_directory(output_dir)?;
    let manifest_path = output_dir.join("manifest.json");
    std::fs::write(&manifest_path, manifest.to_json())?;
    println!("Manifest: {}", manifest_path.display());

    println!("Sharding complete!");
    Ok(())
}

/// Save the decoder as individual per-layer shard files.
///
/// Produces 28 files:
/// - `decoder_embeddings.mpk.gz` — token embedding table
/// - `decoder_layer_00.mpk.gz` through `decoder_layer_25.mpk.gz` — transformer layers
/// - `decoder_norm.mpk.gz` — final RMS normalization
///
/// RoPE is skipped (computed from config, no learned parameters).
pub fn save_streamed_decoder_shards<B: Backend>(
    decoder: LanguageModel<B>,
    output_dir: &Path,
) -> Result<()> {
    let parts = decoder.into_parts();
    let recorder = NamedMpkGzFileRecorder::<FullPrecisionSettings>::default();

    // Save embeddings
    println!("  Saving decoder_embeddings...");
    parts
        .tok_embeddings
        .save_file(output_dir.join("decoder_embeddings"), &recorder)
        .map_err(|e| anyhow::anyhow!("Failed to save decoder embeddings: {}", e))?;
    report_size(&output_dir.join("decoder_embeddings.mpk.gz"));

    // Save each layer individually
    for (i, layer) in parts.layers.into_iter().enumerate() {
        let name = format!("decoder_layer_{:02}", i);
        println!("  Saving {}...", name);
        layer
            .save_file(output_dir.join(&name), &recorder)
            .map_err(|e| anyhow::anyhow!("Failed to save {}: {}", name, e))?;
        report_size(&output_dir.join(format!("{}.mpk.gz", name)));
    }

    // Save norm
    println!("  Saving decoder_norm...");
    parts
        .norm
        .save_file(output_dir.join("decoder_norm"), &recorder)
        .map_err(|e| anyhow::anyhow!("Failed to save decoder norm: {}", e))?;
    report_size(&output_dir.join("decoder_norm.mpk.gz"));

    Ok(())
}

/// Load the encoder phase (encoder + adapter) from shard files.
///
/// Returns the loaded components needed for audio encoding.
pub fn load_encoder_phase<B: Backend>(
    shard_dir: &Path,
    config: &VoxtralModelConfig,
    device: &B::Device,
) -> Result<(AudioEncoder<B>, AudioLanguageAdapter<B>)> {
    let recorder = NamedMpkGzFileRecorder::<FullPrecisionSettings>::default();

    println!("Loading encoder shard...");
    let encoder = config.encoder.init::<B>(device);
    let encoder = encoder
        .load_file(shard_dir.join("encoder"), &recorder, device)
        .map_err(|e| anyhow::anyhow!("Failed to load encoder shard: {}", e))?;

    println!("Loading adapter shard...");
    let adapter = config.adapter.init::<B>(device);
    let adapter = adapter
        .load_file(shard_dir.join("adapter"), &recorder, device)
        .map_err(|e| anyhow::anyhow!("Failed to load adapter shard: {}", e))?;

    Ok((encoder, adapter))
}

/// Load the decoder phase from shard file.
pub fn load_decoder_phase<B: Backend>(
    shard_dir: &Path,
    config: &VoxtralModelConfig,
    device: &B::Device,
) -> Result<LanguageModel<B>> {
    let recorder = NamedMpkGzFileRecorder::<FullPrecisionSettings>::default();

    println!("Loading decoder shard...");
    let decoder = config.decoder.init::<B>(device);
    let decoder = decoder
        .load_file(shard_dir.join("decoder"), &recorder, device)
        .map_err(|e| anyhow::anyhow!("Failed to load decoder shard: {}", e))?;

    Ok(decoder)
}

/// Run the encoder phase: mel → encoder → reshape → adapter → audio_embeds.
///
/// Returns the audio embeddings as a flat Vec<f32> plus the shape [batch, seq, dim].
pub fn run_encoder_phase<B: Backend>(
    encoder: &AudioEncoder<B>,
    adapter: &AudioLanguageAdapter<B>,
    mel: burn::tensor::Tensor<B, 3>,
    reshape_factor: usize,
) -> (Vec<f32>, [usize; 3]) {
    let encoder_out = encoder.forward(mel, 0);
    let reshaped = reshape_encoder_output(encoder_out, reshape_factor);
    let audio_embeds = adapter.forward(reshaped);

    let shape = audio_embeds.dims();
    let data = audio_embeds.into_data().to_vec::<f32>().unwrap();
    (data, shape)
}

fn report_size(path: &Path) {
    if path.exists() {
        if let Ok(meta) = std::fs::metadata(path) {
            println!("  {} -> {:.2} GB", path.display(), meta.len() as f64 / 1e9);
        }
    }
}

/// Decoder shard format in the manifest.
#[derive(Debug, Clone, Copy, PartialEq, Eq)]
pub enum DecoderFormat {
    /// Single monolithic `decoder.mpk.gz` file.
    Monolithic,
    /// Per-layer files: embeddings + N layers + norm.
    Streamed { n_layers: usize },
}

/// Manifest for sharded model loading.
#[derive(Debug, Clone)]
pub struct ShardManifest {
    pub shards: Vec<ShardInfo>,
    pub decoder_format: DecoderFormat,
}

/// Information about a single shard file.
#[derive(Debug, Clone)]
pub struct ShardInfo {
    pub shard_type: PhasedShard,
    pub filename: String,
    pub size: u64,
}

impl ShardManifest {
    /// Create a manifest from a shard directory.
    ///
    /// Auto-detects whether the decoder uses monolithic or streamed format
    /// by checking for `decoder_embeddings.mpk.gz`.
    pub fn from_directory<P: AsRef<Path>>(dir: P) -> Result<Self> {
        let dir = dir.as_ref();
        let mut shards = Vec::new();

        // Encoder and adapter are always monolithic
        for shard_type in &[PhasedShard::Encoder, PhasedShard::Adapter] {
            let filename = format!("{}.mpk.gz", shard_type.filename());
            let path = dir.join(&filename);

            if path.exists() {
                let size = std::fs::metadata(&path)?.len();
                shards.push(ShardInfo {
                    shard_type: *shard_type,
                    filename,
                    size,
                });
            }
        }

        // Detect decoder format
        let streamed_marker = dir.join("decoder_embeddings.mpk.gz");
        let decoder_format = if streamed_marker.exists() {
            // Streamed: count layer files
            let mut n_layers = 0;

            // Embeddings
            add_file_shard(
                &mut shards,
                dir,
                "decoder_embeddings.mpk.gz",
                PhasedShard::Decoder,
            )?;

            // Layers
            loop {
                let layer_file = format!("decoder_layer_{:02}.mpk.gz", n_layers);
                if !dir.join(&layer_file).exists() {
                    break;
                }
                add_file_shard(&mut shards, dir, &layer_file, PhasedShard::Decoder)?;
                n_layers += 1;
            }

            // Norm
            add_file_shard(
                &mut shards,
                dir,
                "decoder_norm.mpk.gz",
                PhasedShard::Decoder,
            )?;

            DecoderFormat::Streamed { n_layers }
        } else {
            // Monolithic
            let filename = "decoder.mpk.gz";
            let path = dir.join(filename);
            if path.exists() {
                let size = std::fs::metadata(&path)?.len();
                shards.push(ShardInfo {
                    shard_type: PhasedShard::Decoder,
                    filename: filename.to_string(),
                    size,
                });
            }
            DecoderFormat::Monolithic
        };

        Ok(Self {
            shards,
            decoder_format,
        })
    }

    /// Total size of all shards.
    pub fn total_size(&self) -> u64 {
        self.shards.iter().map(|s| s.size).sum()
    }

    /// Generate JSON manifest for web loading.
    pub fn to_json(&self) -> String {
        let shards_json: Vec<String> = self
            .shards
            .iter()
            .map(|s| {
                format!(
                    r#"    {{"type": "{}", "filename": "{}", "size": {}}}"#,
                    s.shard_type.filename(),
                    s.filename,
                    s.size
                )
            })
            .collect();

        let (format_str, layers_field) = match self.decoder_format {
            DecoderFormat::Monolithic => ("monolithic".to_string(), String::new()),
            DecoderFormat::Streamed { n_layers } => (
                "streamed".to_string(),
                format!(",\n  \"decoder_layers\": {}", n_layers),
            ),
        };

        format!(
            r#"{{
  "total_size": {},
  "decoder_format": "{}"{},
  "shards": [
{}
  ]
}}"#,
            self.total_size(),
            format_str,
            layers_field,
            shards_json.join(",\n")
        )
    }
}

fn add_file_shard(
    shards: &mut Vec<ShardInfo>,
    dir: &Path,
    filename: &str,
    shard_type: PhasedShard,
) -> Result<()> {
    let path = dir.join(filename);
    if path.exists() {
        let size = std::fs::metadata(&path)?.len();
        shards.push(ShardInfo {
            shard_type,
            filename: filename.to_string(),
            size,
        });
    }
    Ok(())
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_phased_shard_types() {
        assert_eq!(PhasedShard::all().len(), 3);
        assert_eq!(PhasedShard::Encoder.filename(), "encoder");
        assert_eq!(PhasedShard::Adapter.filename(), "adapter");
        assert_eq!(PhasedShard::Decoder.filename(), "decoder");
    }
}
